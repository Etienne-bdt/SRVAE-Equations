@misc{gatopoulos2020Superresolution,
  title = {Super-Resolution {{Variational Auto-Encoders}}},
  author = {Gatopoulos, Ioannis and Stol, Maarten and Tomczak, Jakub M.},
  year = {2020},
  month = jun,
  number = {arXiv:2006.05218},
  eprint = {2006.05218},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.05218},
  urldate = {2025-05-15},
  abstract = {The framework of variational autoencoders (VAEs) provides a principled method for jointly learning latent-variable models and corresponding inference models. However, the main drawback of this approach is the blurriness of the generated images. Some studies link this effect to the objective function, namely, the (negative) log-likelihood (nll). Here, we propose to enhance VAEs by adding a random variable that is a downscaled version of the original image and still use the log-likelihood function as the learning objective. Further, by providing the downscaled image as an input to the decoder, it can be used in a manner similar to the superresolution. We present empirically that the proposed approach performs comparably to VAEs in terms of the nll, but it obtains a better Fr{\'e}chet Inception Distance (FID) score in data synthesis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
}

@inproceedings{hershey2007Approximating,
  title = {Approximating the {{Kullback Leibler Divergence Between Gaussian Mixture Models}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  author = {Hershey, John R. and Olsen, Peder A.},
  year = {2007},
  month = apr,
  volume = {4},
  pages = {IV-317-IV-320},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2007.366913},
  urldate = {2025-05-21},
  abstract = {The Kullback Leibler (KL) divergence is a widely used tool in statistics and pattern recognition. The KL divergence between two Gaussian mixture models (GMMs) is frequently needed in the fields of speech and image recognition. Unfortunately the KL divergence between two GMMs is not analytically tractable, nor does any efficient computational algorithm exist. Some techniques cope with this problem by replacing the KL divergence with other functions that can be computed efficiently. We introduce two new methods, the variational approximation and the variational upper bound, and compare them to existing methods. We discuss seven different techniques in total and weigh the benefits of each one against the others. To conclude we evaluate the performance of each one through numerical experiments.},
  keywords = {Algorithm design and analysis,Entropy,gaussian mixture models,Image recognition,Kullback Leibler divergence,Monte Carlo methods,Pattern recognition,Probability density function,Speech,Statistical distributions,Statistics,unscented transformation,Upper bound,variational methods},
  file = {/home/disc/e.bardet/Zotero/storage/R8VV9EPR/Hershey et Olsen - 2007 - Approximating the Kullback Leibler Divergence Between Gaussian Mixture Models.pdf}
}

@inproceedings{rybkin2021simple,
  title={Simple and effective VAE training with calibrated decoders},
  author={Rybkin, Oleh and Daniilidis, Kostas and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={9179--9189},
  year={2021},
  organization={PMLR}
}
