@misc{gatopoulos2020Superresolution,
  title = {Super-Resolution {{Variational Auto-Encoders}}},
  author = {Gatopoulos, Ioannis and Stol, Maarten and Tomczak, Jakub M.},
  year = {2020},
  month = jun,
  number = {arXiv:2006.05218},
  eprint = {2006.05218},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.05218},
  urldate = {2025-05-15},
  abstract = {The framework of variational autoencoders (VAEs) provides a principled method for jointly learning latent-variable models and corresponding inference models. However, the main drawback of this approach is the blurriness of the generated images. Some studies link this effect to the objective function, namely, the (negative) log-likelihood (nll). Here, we propose to enhance VAEs by adding a random variable that is a downscaled version of the original image and still use the log-likelihood function as the learning objective. Further, by providing the downscaled image as an input to the decoder, it can be used in a manner similar to the superresolution. We present empirically that the proposed approach performs comparably to VAEs in terms of the nll, but it obtains a better Fr{\'e}chet Inception Distance (FID) score in data synthesis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
}
